{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d19f3ac8",
   "metadata": {},
   "source": [
    "Weather Data Downloader\n",
    "\n",
    "This script downloads sample weather data from NOAA's Global Historical Climatology Network (GHCN),\n",
    "processes it, and loads it into a PostgreSQL database for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e45158c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/calvin/projects/zoomcamp/de-zoomcamp-2025/.venv/lib/python3.9/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in /home/calvin/projects/zoomcamp/de-zoomcamp-2025/.venv/lib/python3.9/site-packages (2.0.2)\n",
      "Collecting requests\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: sqlalchemy in /home/calvin/projects/zoomcamp/de-zoomcamp-2025/.venv/lib/python3.9/site-packages (2.0.40)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.9.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/calvin/projects/zoomcamp/de-zoomcamp-2025/.venv/lib/python3.9/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/calvin/projects/zoomcamp/de-zoomcamp-2025/.venv/lib/python3.9/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/calvin/projects/zoomcamp/de-zoomcamp-2025/.venv/lib/python3.9/site-packages (from pandas) (2025.2)\n",
      "Collecting charset-normalizer<4,>=2 (from requests)\n",
      "  Downloading charset_normalizer-3.4.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
      "Collecting idna<4,>=2.5 (from requests)\n",
      "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests)\n",
      "  Downloading urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests)\n",
      "  Downloading certifi-2025.4.26-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: greenlet>=1 in /home/calvin/projects/zoomcamp/de-zoomcamp-2025/.venv/lib/python3.9/site-packages (from sqlalchemy) (3.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /home/calvin/projects/zoomcamp/de-zoomcamp-2025/.venv/lib/python3.9/site-packages (from sqlalchemy) (4.13.2)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.57.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (102 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.7-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/calvin/projects/zoomcamp/de-zoomcamp-2025/.venv/lib/python3.9/site-packages (from matplotlib) (25.0)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Downloading pillow-11.2.1-cp39-cp39-manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting importlib-resources>=3.2.0 (from matplotlib)\n",
      "  Downloading importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/calvin/projects/zoomcamp/de-zoomcamp-2025/.venv/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.21.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/calvin/projects/zoomcamp/de-zoomcamp-2025/.venv/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading charset_normalizer-3.4.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (146 kB)\n",
      "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
      "Downloading matplotlib-3.9.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m123.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Downloading certifi-2025.4.26-py3-none-any.whl (159 kB)\n",
      "Downloading contourpy-1.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (321 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.57.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Downloading kiwisolver-1.4.7-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m97.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pillow-11.2.1-cp39-cp39-manylinux_2_28_x86_64.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Installing collected packages: urllib3, pyparsing, pillow, kiwisolver, importlib-resources, idna, fonttools, cycler, contourpy, charset-normalizer, certifi, requests, matplotlib, seaborn\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/14\u001b[0m [seaborn]3/14\u001b[0m [seaborn]ib]\n",
      "\u001b[1A\u001b[2KSuccessfully installed certifi-2025.4.26 charset-normalizer-3.4.1 contourpy-1.3.0 cycler-0.12.1 fonttools-4.57.0 idna-3.10 importlib-resources-6.5.2 kiwisolver-1.4.7 matplotlib-3.9.4 pillow-11.2.1 pyparsing-3.2.3 requests-2.32.3 seaborn-0.13.2 urllib3-2.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy requests sqlalchemy matplotlib seaborn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "540eeb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import gzip\n",
    "import shutil\n",
    "from sqlalchemy import create_engine, text\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import io\n",
    "import time\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4be18872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger('weather_data_downloader')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0361d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database connection parameters\n",
    "DB_USER = 'postgres'\n",
    "DB_PASSWORD = 'postgres'\n",
    "DB_HOST = 'localhost'  # Use 'postgres' if running within Docker network\n",
    "DB_PORT = '5432'\n",
    "DB_NAME = 'weather_data'\n",
    "\n",
    "# Create connection string\n",
    "connection_string = f\"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    "\n",
    "# Data storage directory\n",
    "DATA_DIR = os.path.join(os.getcwd(), 'data')\n",
    "os.makedirs(DATA_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e67b4206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url, local_path):\n",
    "    \"\"\"\n",
    "    Download a file from a URL to a local path\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Downloading {url} to {local_path}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Stream the download to handle large files efficiently\n",
    "        with requests.get(url, stream=True) as r:\n",
    "            r.raise_for_status()\n",
    "            with open(local_path, 'wb') as f:\n",
    "                for chunk in r.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        logger.info(f\"Download completed in {elapsed_time:.2f} seconds\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error downloading file: {e}\")\n",
    "        return False\n",
    "\n",
    "def extract_gz_file(gz_path, output_path):\n",
    "    \"\"\"\n",
    "    Extract a gzipped file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Extracting {gz_path} to {output_path}\")\n",
    "        with gzip.open(gz_path, 'rb') as f_in:\n",
    "            with open(output_path, 'wb') as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "        logger.info(\"Extraction completed\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error extracting file: {e}\")\n",
    "        return False\n",
    "\n",
    "def download_ghcn_stations():\n",
    "    \"\"\"\n",
    "    Download and process the GHCN stations inventory file\n",
    "    \"\"\"\n",
    "    # URL for the stations inventory file\n",
    "    stations_url = \"https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt\"\n",
    "    \n",
    "    # Local path for the downloaded file\n",
    "    stations_file = os.path.join(DATA_DIR, \"ghcnd-stations.txt\")\n",
    "    \n",
    "    # Download the file\n",
    "    if not download_file(stations_url, stations_file):\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # The stations file has a fixed width format, so we need to specify column widths\n",
    "        # Format details: https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/readme.txt\n",
    "        colspecs = [\n",
    "            (0, 11),    # ID\n",
    "            (12, 20),   # LATITUDE\n",
    "            (21, 30),   # LONGITUDE\n",
    "            (31, 37),   # ELEVATION\n",
    "            (38, 68),   # STATION NAME\n",
    "            (69, 75),   # STATE (optional)\n",
    "            (76, 79)    # COUNTRY\n",
    "        ]\n",
    "        \n",
    "        # Column names\n",
    "        columns = ['station_id', 'latitude', 'longitude', 'elevation', 'name', 'state', 'country']\n",
    "        \n",
    "        # Read the fixed width file\n",
    "        logger.info(\"Processing stations file\")\n",
    "        df = pd.read_fwf(stations_file, colspecs=colspecs, names=columns)\n",
    "        \n",
    "        # Clean up the data\n",
    "        df['latitude'] = pd.to_numeric(df['latitude'], errors='coerce')\n",
    "        df['longitude'] = pd.to_numeric(df['longitude'], errors='coerce')\n",
    "        df['elevation'] = pd.to_numeric(df['elevation'], errors='coerce')\n",
    "        \n",
    "        # Fill missing values for state\n",
    "        df['state'] = df['state'].fillna('')\n",
    "        \n",
    "        # Trim whitespace from string columns\n",
    "        for col in ['station_id', 'name', 'state', 'country']:\n",
    "            df[col] = df[col].str.strip()\n",
    "        \n",
    "        logger.info(f\"Processed {len(df)} stations\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing stations file: {e}\")\n",
    "        return None\n",
    "\n",
    "def download_ghcn_data(year, month=None, sample_size=100):\n",
    "    \"\"\"\n",
    "    Download GHCN daily data for a specific year and optionally filter by month\n",
    "    \n",
    "    Args:\n",
    "        year (int): Year to download data for\n",
    "        month (int, optional): Month to filter data by (1-12)\n",
    "        sample_size (int, optional): Number of random stations to sample\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: Processed weather data\n",
    "    \"\"\"\n",
    "    # URL for the yearly data file\n",
    "    data_url = f\"https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/by_year/{year}.csv.gz\"\n",
    "    \n",
    "    # Local paths for the downloaded and extracted files\n",
    "    gz_file = os.path.join(DATA_DIR, f\"{year}.csv.gz\")\n",
    "    csv_file = os.path.join(DATA_DIR, f\"{year}.csv\")\n",
    "    \n",
    "    # Download the file\n",
    "    if not download_file(data_url, gz_file):\n",
    "        return None\n",
    "    \n",
    "    # Extract the file\n",
    "    if not extract_gz_file(gz_file, csv_file):\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # The data file contains millions of records, so we'll read in chunks\n",
    "        logger.info(f\"Processing {year} data file\")\n",
    "        \n",
    "        # Column names based on GHCN documentation\n",
    "        columns = ['station_id', 'date', 'element', 'value', 'mflag', 'qflag', 'sflag', 'obstime']\n",
    "        \n",
    "        # Read a sample of the data\n",
    "        # If month is specified, filter for that month\n",
    "        if month:\n",
    "            month_str = f\"{month:02d}\"\n",
    "            \n",
    "            # Read the entire file first (this is inefficient but simpler for demonstration)\n",
    "            df = pd.read_csv(csv_file, names=columns, header=None)\n",
    "            \n",
    "            # Extract year and month from date column\n",
    "            df['year'] = df['date'].astype(str).str[:4]\n",
    "            df['month'] = df['date'].astype(str).str[4:6]\n",
    "            \n",
    "            # Filter for the specified month\n",
    "            df = df[df['month'] == month_str]\n",
    "            \n",
    "            # Sample a subset of stations\n",
    "            if sample_size:\n",
    "                unique_stations = df['station_id'].unique()\n",
    "                if len(unique_stations) > sample_size:\n",
    "                    sampled_stations = np.random.choice(unique_stations, sample_size, replace=False)\n",
    "                    df = df[df['station_id'].isin(sampled_stations)]\n",
    "            \n",
    "            logger.info(f\"Processed {len(df)} records for {year}-{month}\")\n",
    "        else:\n",
    "            # If no month specified, just take a random sample of rows\n",
    "            # Count lines in file\n",
    "            with open(csv_file, 'r') as f:\n",
    "                line_count = sum(1 for _ in f)\n",
    "            \n",
    "            # Take a random sample\n",
    "            sample_indices = np.random.choice(line_count, min(100000, line_count), replace=False)\n",
    "            sample_indices = sorted(sample_indices)\n",
    "            \n",
    "            # Read only the sampled rows\n",
    "            df = pd.read_csv(csv_file, skiprows=lambda i: i not in sample_indices, names=columns, header=None)\n",
    "            logger.info(f\"Sampled {len(df)} records from {year}\")\n",
    "        \n",
    "        # Convert date string to date object\n",
    "        df['date'] = pd.to_datetime(df['date'], format='%Y%m%d')\n",
    "        \n",
    "        # Convert value to float and handle the scale factor\n",
    "        # GHCN stores temperatures in tenths of degrees C and precipitation in tenths of mm\n",
    "        df['value'] = pd.to_numeric(df['value'], errors='coerce') / 10.0\n",
    "        \n",
    "        # Clean up the data\n",
    "        for col in ['mflag', 'qflag', 'sflag', 'obstime']:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].fillna('')\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing {year} data file: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_database_schema(engine):\n",
    "    \"\"\"\n",
    "    Create the database schema for the weather data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(\"Creating database schema\")\n",
    "        \n",
    "        # Create stations table\n",
    "        create_stations_table = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS weather_stations (\n",
    "            station_id VARCHAR(20) PRIMARY KEY,\n",
    "            latitude FLOAT,\n",
    "            longitude FLOAT,\n",
    "            elevation FLOAT,\n",
    "            name VARCHAR(100),\n",
    "            state VARCHAR(50),\n",
    "            country VARCHAR(50)\n",
    "        );\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create weather data table\n",
    "        create_weather_table = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS weather_readings (\n",
    "            id SERIAL PRIMARY KEY,\n",
    "            station_id VARCHAR(20) REFERENCES weather_stations(station_id),\n",
    "            date DATE,\n",
    "            element VARCHAR(10),\n",
    "            value FLOAT,\n",
    "            mflag VARCHAR(1),\n",
    "            qflag VARCHAR(1),\n",
    "            sflag VARCHAR(1),\n",
    "            obstime VARCHAR(4)\n",
    "        );\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create index on station_id, date, and element\n",
    "        create_index = \"\"\"\n",
    "        CREATE INDEX IF NOT EXISTS idx_station_date_element \n",
    "        ON weather_readings (station_id, date, element);\n",
    "        \"\"\"\n",
    "        \n",
    "        # Execute the SQL statements\n",
    "        with engine.connect() as conn:\n",
    "            conn.execute(text(create_stations_table))\n",
    "            conn.execute(text(create_weather_table))\n",
    "            conn.execute(text(create_index))\n",
    "            conn.commit()\n",
    "        \n",
    "        logger.info(\"Database schema created\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating database schema: {e}\")\n",
    "        return False\n",
    "\n",
    "def load_data_to_db(engine, stations_df, weather_df):\n",
    "    \"\"\"\n",
    "    Load the processed data into the database\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(\"Loading data to database\")\n",
    "        \n",
    "        # Load stations data\n",
    "        if stations_df is not None:\n",
    "            stations_df.to_sql('weather_stations', engine, if_exists='replace', index=False)\n",
    "            logger.info(f\"Loaded {len(stations_df)} stations to database\")\n",
    "        \n",
    "        # Load weather data\n",
    "        if weather_df is not None:\n",
    "            # To avoid primary key conflicts, we'll use append mode and let the database assign IDs\n",
    "            weather_df.to_sql('weather_readings', engine, if_exists='append', index=False)\n",
    "            logger.info(f\"Loaded {len(weather_df)} weather readings to database\")\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading data to database: {e}\")\n",
    "        return False\n",
    "\n",
    "def explore_data(engine):\n",
    "    \"\"\"\n",
    "    Run basic exploratory queries on the data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(\"Running exploratory queries\")\n",
    "        \n",
    "        # Query for station count\n",
    "        station_query = \"SELECT COUNT(*) FROM weather_stations;\"\n",
    "        with engine.connect() as conn:\n",
    "            result = conn.execute(text(station_query))\n",
    "            station_count = result.scalar()\n",
    "            logger.info(f\"Total stations in database: {station_count}\")\n",
    "        \n",
    "        # Query for weather readings count\n",
    "        readings_query = \"SELECT COUNT(*) FROM weather_readings;\"\n",
    "        with engine.connect() as conn:\n",
    "            result = conn.execute(text(readings_query))\n",
    "            readings_count = result.scalar()\n",
    "            logger.info(f\"Total weather readings in database: {readings_count}\")\n",
    "        \n",
    "        # Query for elements distribution\n",
    "        elements_query = \"\"\"\n",
    "        SELECT element, COUNT(*) as count\n",
    "        FROM weather_readings\n",
    "        GROUP BY element\n",
    "        ORDER BY count DESC;\n",
    "        \"\"\"\n",
    "        with engine.connect() as conn:\n",
    "            result = conn.execute(text(elements_query))\n",
    "            logger.info(\"Elements distribution:\")\n",
    "            for row in result:\n",
    "                logger.info(f\"  {row[0]}: {row[1]}\")\n",
    "        \n",
    "        # Query for date range\n",
    "        date_query = \"\"\"\n",
    "        SELECT MIN(date), MAX(date)\n",
    "        FROM weather_readings;\n",
    "        \"\"\"\n",
    "        with engine.connect() as conn:\n",
    "            result = conn.execute(text(date_query))\n",
    "            min_date, max_date = result.fetchone()\n",
    "            logger.info(f\"Date range: {min_date} to {max_date}\")\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error exploring data: {e}\")\n",
    "        return False\n",
    "\n",
    "def generate_sample_visualizations(engine):\n",
    "    \"\"\"\n",
    "    Generate some sample visualizations from the data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(\"Generating sample visualizations\")\n",
    "        \n",
    "        # Query for temperature data\n",
    "        temp_query = \"\"\"\n",
    "        SELECT \n",
    "            r.date, \n",
    "            s.name as station_name,\n",
    "            s.latitude,\n",
    "            s.longitude,\n",
    "            r.element, \n",
    "            r.value\n",
    "        FROM \n",
    "            weather_readings r\n",
    "        JOIN \n",
    "            weather_stations s ON r.station_id = s.station_id\n",
    "        WHERE \n",
    "            r.element IN ('TMAX', 'TMIN')\n",
    "            AND r.qflag = ''\n",
    "        ORDER BY \n",
    "            r.date, s.name, r.element;\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create a dataframe from the query\n",
    "        temp_df = pd.read_sql(temp_query, engine)\n",
    "        \n",
    "        if len(temp_df) == 0:\n",
    "            logger.warning(\"No temperature data found for visualization\")\n",
    "            return False\n",
    "        \n",
    "        # Create output directory for visualizations\n",
    "        vis_dir = os.path.join(os.getcwd(), 'visualizations')\n",
    "        os.makedirs(vis_dir, exist_ok=True)\n",
    "        \n",
    "        # 1. Temperature time series for a few stations\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Get the top 5 stations with the most data\n",
    "        top_stations = temp_df['station_name'].value_counts().head(5).index\n",
    "        \n",
    "        # Plot for each station\n",
    "        for station in top_stations:\n",
    "            station_data = temp_df[temp_df['station_name'] == station]\n",
    "            \n",
    "            # Get TMAX and TMIN data\n",
    "            tmax_data = station_data[station_data['element'] == 'TMAX']\n",
    "            tmin_data = station_data[station_data['element'] == 'TMIN']\n",
    "            \n",
    "            if len(tmax_data) > 0:\n",
    "                plt.plot(tmax_data['date'], tmax_data['value'], 'o-', label=f\"{station} (Max)\")\n",
    "            \n",
    "            if len(tmin_data) > 0:\n",
    "                plt.plot(tmin_data['date'], tmin_data['value'], 'o--', label=f\"{station} (Min)\")\n",
    "        \n",
    "        plt.title('Temperature Trends by Station', fontsize=16)\n",
    "        plt.xlabel('Date', fontsize=12)\n",
    "        plt.ylabel('Temperature (°C)', fontsize=12)\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(vis_dir, 'temperature_trends.png'))\n",
    "        \n",
    "        # 2. Distribution of temperature values\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Plot histograms for TMAX and TMIN\n",
    "        tmax_data = temp_df[temp_df['element'] == 'TMAX']\n",
    "        tmin_data = temp_df[temp_df['element'] == 'TMIN']\n",
    "        \n",
    "        if len(tmax_data) > 0:\n",
    "            sns.histplot(tmax_data['value'], kde=True, label='TMAX')\n",
    "        \n",
    "        if len(tmin_data) > 0:\n",
    "            sns.histplot(tmin_data['value'], kde=True, label='TMIN')\n",
    "        \n",
    "        plt.title('Distribution of Temperature Values', fontsize=16)\n",
    "        plt.xlabel('Temperature (°C)', fontsize=12)\n",
    "        plt.ylabel('Frequency', fontsize=12)\n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join(vis_dir, 'temperature_distribution.png'))\n",
    "        \n",
    "        # 3. Temperature by latitude (if we have geographic data)\n",
    "        if 'latitude' in temp_df.columns and not temp_df['latitude'].isna().all():\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            \n",
    "            # Average temperature by latitude\n",
    "            temp_by_lat = temp_df.groupby(['latitude', 'element'])['value'].mean().reset_index()\n",
    "            \n",
    "            # Plot for TMAX and TMIN\n",
    "            tmax_by_lat = temp_by_lat[temp_by_lat['element'] == 'TMAX']\n",
    "            tmin_by_lat = temp_by_lat[temp_by_lat['element'] == 'TMIN']\n",
    "            \n",
    "            if len(tmax_by_lat) > 0:\n",
    "                plt.scatter(tmax_by_lat['latitude'], tmax_by_lat['value'], label='TMAX', alpha=0.7)\n",
    "            \n",
    "            if len(tmin_by_lat) > 0:\n",
    "                plt.scatter(tmin_by_lat['latitude'], tmin_by_lat['value'], label='TMIN', alpha=0.7)\n",
    "            \n",
    "            plt.title('Temperature by Latitude', fontsize=16)\n",
    "            plt.xlabel('Latitude', fontsize=12)\n",
    "            plt.ylabel('Average Temperature (°C)', fontsize=12)\n",
    "            plt.legend()\n",
    "            plt.savefig(os.path.join(vis_dir, 'temperature_by_latitude.png'))\n",
    "        \n",
    "        logger.info(f\"Visualizations saved to {vis_dir}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error generating visualizations: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b95bd2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to download and process weather data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(\"Starting weather data download and processing\")\n",
    "        \n",
    "        # Create SQLAlchemy engine\n",
    "        engine = create_engine(connection_string)\n",
    "        \n",
    "        # Create database schema\n",
    "        create_database_schema(engine)\n",
    "        \n",
    "        # Download and process stations data\n",
    "        stations_df = download_ghcn_stations()\n",
    "        \n",
    "        # Download weather data for a recent year and month\n",
    "        # You can change the year and month as needed\n",
    "        current_year = datetime.now().year\n",
    "        current_month = datetime.now().month\n",
    "        \n",
    "        # Use previous month to ensure data is available\n",
    "        if current_month == 1:\n",
    "            year = current_year - 1\n",
    "            month = 12\n",
    "        else:\n",
    "            year = current_year\n",
    "            month = current_month - 1\n",
    "        \n",
    "        weather_df = download_ghcn_data(year, month, sample_size=50)\n",
    "        \n",
    "        # Load data to database\n",
    "        load_data_to_db(engine, stations_df, weather_df)\n",
    "        \n",
    "        # Run exploratory queries\n",
    "        explore_data(engine)\n",
    "        \n",
    "        # Generate visualizations\n",
    "        generate_sample_visualizations(engine)\n",
    "        \n",
    "        logger.info(\"Weather data processing completed successfully\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in main function: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "707e80d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 16:18:44,332 - weather_data_downloader - INFO - Starting weather data download and processing\n",
      "2025-04-27 16:18:44,371 - weather_data_downloader - INFO - Creating database schema\n",
      "2025-04-27 16:18:44,387 - weather_data_downloader - INFO - Database schema created\n",
      "2025-04-27 16:18:44,388 - weather_data_downloader - INFO - Downloading https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt to /home/calvin/projects/zoomcamp/de-zoomcamp-2025/weather-analysis/jupyter/notebooks/data/ghcnd-stations.txt\n",
      "2025-04-27 16:19:14,785 - weather_data_downloader - ERROR - Error downloading file: 503 Server Error: Service Unavailable for url: https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt\n",
      "2025-04-27 16:19:14,786 - weather_data_downloader - INFO - Downloading https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/by_year/2025.csv.gz to /home/calvin/projects/zoomcamp/de-zoomcamp-2025/weather-analysis/jupyter/notebooks/data/2025.csv.gz\n",
      "2025-04-27 16:19:45,175 - weather_data_downloader - ERROR - Error downloading file: 503 Server Error: Service Unavailable for url: https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/by_year/2025.csv.gz\n",
      "2025-04-27 16:19:45,176 - weather_data_downloader - INFO - Loading data to database\n",
      "2025-04-27 16:19:45,176 - weather_data_downloader - INFO - Running exploratory queries\n",
      "2025-04-27 16:19:45,179 - weather_data_downloader - INFO - Total stations in database: 4\n",
      "2025-04-27 16:19:45,181 - weather_data_downloader - INFO - Total weather readings in database: 84\n",
      "2025-04-27 16:19:45,182 - weather_data_downloader - INFO - Elements distribution:\n",
      "2025-04-27 16:19:45,183 - weather_data_downloader - INFO -   TMAX: 28\n",
      "2025-04-27 16:19:45,183 - weather_data_downloader - INFO -   TMIN: 28\n",
      "2025-04-27 16:19:45,184 - weather_data_downloader - INFO -   PRCP: 28\n",
      "2025-04-27 16:19:45,186 - weather_data_downloader - INFO - Date range: 2025-04-21 to 2025-04-27\n",
      "2025-04-27 16:19:45,186 - weather_data_downloader - INFO - Generating sample visualizations\n",
      "2025-04-27 16:19:45,193 - weather_data_downloader - ERROR - Error generating visualizations: (psycopg2.errors.UndefinedColumn) column s.name does not exist\n",
      "LINE 4:             s.name as station_name,\n",
      "                    ^\n",
      "HINT:  Perhaps you meant to reference the column \"r.date\".\n",
      "\n",
      "[SQL: \n",
      "        SELECT \n",
      "            r.date, \n",
      "            s.name as station_name,\n",
      "            s.latitude,\n",
      "            s.longitude,\n",
      "            r.element, \n",
      "            r.value\n",
      "        FROM \n",
      "            weather_readings r\n",
      "        JOIN \n",
      "            weather_stations s ON r.station_id = s.station_id\n",
      "        WHERE \n",
      "            r.element IN ('TMAX', 'TMIN')\n",
      "            AND r.qflag = ''\n",
      "        ORDER BY \n",
      "            r.date, s.name, r.element;\n",
      "        ]\n",
      "(Background on this error at: https://sqlalche.me/e/20/f405)\n",
      "2025-04-27 16:19:45,193 - weather_data_downloader - INFO - Weather data processing completed successfully\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
